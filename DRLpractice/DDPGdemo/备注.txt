1，文件结构：
训练ddpg模型：
【main.py】 ： 启动模型训练的主函数  (训练输出的evaluation图在word文档里,但不建议加入到报告文档里)
【DDPG.py】 ： 模型结构，适当调参即可；     主要参考TD3源码：https://github.com/sfujim/TD3（对应的OurDDPG，因为根据DDPG原论文的参数设定，不太work）
【utils.py】 ： ReplayBuffer，存储每一步情况作为训练数据；为了让模型跑起来，对数据进行了简单的归一化（但是方式是不太对的，只是为了让模型生效）后面需要修改
【env.py】 ： 游戏环境（无人机飞行场景）设置，奖励函数在这里进行修改

用于展示（实际上不太规范，还有很多需要琢磨的点）：
【model文件夹】：存储有通过上面的代码训练好的ddpg模型参数，如果要调参并重新训练，可以先把里面的文件删了或者修改main中的文件名
【loadDDPG.py】 ： 将训练好的ddpg加载出来，在testEnv下测试是否可以成功导航
注:seed设置不同的值,随机场景下的效果会有影响,所以可以修改seed,然后选择随机场景下效果比较差的来进行展示(有点类似于作假...)

2，绘制图像的问题与一点小建议：
因为用的pyglet，寻路过程不太方便展示在文本报告里，或许可以把每一步的x、y坐标记录下来，再用pyplot重新画一下图？
答辩的话，或许可以用测试里面的loadDDPG.py直接跑一下？或者录个视频？

3，其他：
代码中的render()可以根据需求添加或删除；print也是，每一步都print出对应信息会很慢，只是为了便于调参

4，模型的问题：
（1）场景设置：奖励函数设置还需要优化；
            障碍物的数量太少；
            基于势场法构建奖励函数来解决稀疏奖励，和直接用传统势场法的区别在哪里？逻辑上还有待商榷；
（2）模型的泛化性：因为场景设置单一且简单，所以在修改了障碍物的位置、数量的情况下(即更换场景)，模型就不太work，因而泛化性是一个大bug；
（3）目前的模型能说明什么：仅能说明用DDPG可以实现避障路径的规划，但效果还不太理想，模型的泛化性也需要进一步实验；


